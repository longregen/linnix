# Environment configuration for Linnix with distilled 3B model
# Source this file: source .env.distilled

# Distilled 3B model configuration
export LLM_ENDPOINT="http://localhost:8090/v1/chat/completions"
export LLM_MODEL="linnix-3b-distilled"

# Optional: Override cognitod host
export COGNITOD_HOST="http://localhost:3000"

# llama.cpp server configuration
export LLAMA_PORT=8090
export LLAMA_CTX=4096
export LLAMA_THREADS=8

echo "âœ… Linnix environment configured for distilled 3B model"
echo "   Model: linnix-3b-distilled (Q5_K_M, 2.1GB)"
echo "   Endpoint: $LLM_ENDPOINT"
echo "   Cognitod: $COGNITOD_HOST"
