name: Build and Publish Docker Images

on:
  push:
    branches:
      - main
    tags:
      - 'v*'
  pull_request:
    branches:
      - main
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  COGNITOD_IMAGE_NAME: linnix-os/cognitod
  LLAMA_IMAGE_NAME: linnix-os/llama-cpp

jobs:
  build-cognitod:
    name: Build Cognitod Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.COGNITOD_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=semver,pattern={{major}}
            type=sha
            type=raw,value=${{ github.sha }}
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          # TODO: ARM64 support blocked by bpf-linker LLVM issues in QEMU
          # See: https://github.com/aya-rs/bpf-linker/issues
          platforms: linux/amd64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            VERSION=${{ github.ref_name }}
            COMMIT=${{ github.sha }}

  build-llama-cpp:
    name: Build llama.cpp Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.LLAMA_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=semver,pattern={{major}}
            type=sha
            type=raw,value=${{ github.sha }}
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./docker/llama-cpp
          file: ./docker/llama-cpp/Dockerfile
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  test-docker-compose:
    name: Test Docker Compose Setup
    runs-on: ubuntu-latest
    needs: [build-cognitod, build-llama-cpp]
    if: github.event_name == 'pull_request' || github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create test config
        run: |
          mkdir -p configs
          cp configs/linnix.toml.example configs/linnix.toml || \
            echo '[runtime]' > configs/linnix.toml

      - name: Start services
        run: |
          # Use locally built images for testing
          export COGNITOD_IMAGE=ghcr.io/${{ env.COGNITOD_IMAGE_NAME }}:${{ github.sha }}
          export LLAMA_IMAGE=ghcr.io/${{ env.LLAMA_IMAGE_NAME }}:${{ github.sha }}
          docker compose up -d

      - name: Wait for services
        run: |
          echo "Waiting for cognitod..."
          COGNITOD_READY=false
          for i in {1..30}; do
            if curl -sf http://localhost:3000/healthz > /dev/null 2>&1; then
              echo "✅ Cognitod is healthy"
              COGNITOD_READY=true
              break
            fi
            echo "Attempt $i/30..."
            sleep 2
          done

          if [ "$COGNITOD_READY" = "false" ]; then
            echo "❌ Cognitod failed to start"
            docker compose logs cognitod
            exit 1
          fi

          echo "Waiting for llama-server..."
          LLAMA_READY=false
          for i in {1..60}; do
            if curl -sf http://localhost:8090/health > /dev/null 2>&1; then
              echo "✅ LLM server is healthy"
              LLAMA_READY=true
              break
            fi
            echo "Attempt $i/60..."
            sleep 2
          done

          if [ "$LLAMA_READY" = "false" ]; then
            echo "⚠️ LLM server not ready (optional service)"
            docker compose logs llama-server
          fi

      - name: Test endpoints
        run: |
          echo "Testing cognitod endpoints..."
          
          # Test healthz
          echo "GET /healthz"
          curl -f http://localhost:3000/healthz || exit 1
          
          # Test status
          echo "GET /status"
          curl -f http://localhost:3000/status | jq || exit 1
          
          # Test processes (may be empty but should return 200)
          echo "GET /processes"
          curl -f http://localhost:3000/processes | jq || exit 1
          
          echo "✅ All cognitod endpoints working"

      - name: Check logs
        if: failure()
        run: |
          echo "=== Cognitod logs ==="
          docker compose logs cognitod
          echo "=== LLM logs ==="
          docker compose logs llama-server

      - name: Cleanup
        if: always()
        run: docker compose down -v
