# Docker Compose override for LLM integration
# Uses official llama.cpp image with Linnix 3B distilled model
#
# Prerequisites:
#   ./setup-llm.sh  (auto-downloads model from Hugging Face)
#
# Or manual setup:
#   mkdir -p models
#   wget https://huggingface.co/parth21shah/linnix-3b-distilled/resolve/main/linnix-3b-distilled-q5_k_m.gguf -P models/
#
# Usage:
#   docker-compose -f docker-compose.yml -f docker-compose.llm.yml up -d

version: '3.8'

services:
  llama-server:
    # Use official llama.cpp image
    image: ghcr.io/ggerganov/llama.cpp:server
    # Mount local models directory (read-only)
    volumes:
      - ./models:/models:ro
    # Run Linnix 3B model
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8090"
      - -m
      - /models/linnix-3b-distilled-q5_k_m.gguf
      - --ctx-size
      - "4096"
      - -t
      - "4"
